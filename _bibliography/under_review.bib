@article{Perez-Lebel2021,
abstract = {BACKGROUND As databases grow larger, it becomes harder to fully control their collection, and they frequently come with missing values: incomplete observations. These large databases are well suited to train machine-learning models, for instance for forecasting or to extract biomarkers in biomedical settings. Such predictive approaches can use discriminative --rather than generative-- modeling, and thus open the door to new missing-values strategies. Yet existing empirical evaluations of strategies to handle missing values have focused on inferential statistics. RESULTS Here we conduct a systematic benchmark of missing-values strategies in predictive models with a focus on health databases: two electronic health record datasets, a population brain imaging one, and a health survey. Using gradient-boosted trees, we compare native support for missing values with simple and state-of-the-art imputation prior to learning. We investigate prediction accuracy and computational time. For prediction after imputation, we find that adding an indicator to express which values have been imputed is important, suggesting that the data are missing not at random. Elaborate missing values imputation can improve prediction compared to simple strategies but requires longer computational time on large data. Learning trees that model missing values --with missing incorporated attribute-- leads to robust, fast, and well-performing predictive modeling. CONCLUSIONS Native support for missing values in supervised machine learning predicts better than state-of-the-art imputation with much less computational cost. When using imputation, it is important to add indicator columns expressing which values have been imputed.},
author = {Perez-Lebel, Alexandre and Varoquaux, Ga{\"{e}}l and {Le Morvan}, Marine and Josse, Julie and Poline, Jean-Baptiste},
journal = {GigaScience},
mendeley-groups = {Ours/Under review},
title = {{Benchmarking missing-values approaches for predictive models on health databases.}},
bibtex_show={true},
abbr={Benchmark}
}
